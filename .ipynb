{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature (°C)</th>\n",
       "      <th>Precipitation (mm)</th>\n",
       "      <th>Conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>14</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Showers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Temperature (°C)    Precipitation (mm)  \\\n",
       "0  2023-11-01                       15                   0.2   \n",
       "1  2023-11-02                       16                   0.0   \n",
       "2  2023-11-03                       14                   2.5   \n",
       "3  2023-11-04                       12                   8.0   \n",
       "4  2023-11-05                       13                   0.0   \n",
       "\n",
       "                  Conditions  \n",
       "0                      Clear  \n",
       "1              Partly Cloudy  \n",
       "2                       Rain  \n",
       "3                    Showers  \n",
       "4                      Clear  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-11-01', periods=30),\n",
    "    'Temperature': [15, 16, 14, 12, 13, 17, 18, 16, 14, 13, 14, 15, 13, 12, 11, 14, 16, 15, 14, 13, 12, 13, 14, 16, 17, 15, 14, 14, 12, 11],\n",
    "    'Precipitation': [0.2, 0.0, 2.5, 8.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 2.0, 0.0],\n",
    "    'Conditions': ['Clear', 'Partly Cloudy', 'Rain', 'Showers', 'Clear', 'Sunny', 'Sunny', 'Partly Cloudy', 'Drizzle', 'Clear', 'Clear', 'Partly Cloudy', 'Showers', 'Rain', 'Clear', 'Sunny', 'Sunny', 'Partly Cloudy', 'Clear', 'Clear', 'Partly Cloudy', 'Sunny', 'Sunny', 'Partly Cloudy', 'Clear', 'Clear', 'Partly Cloudy', 'Drizzle', 'Rain', 'Clear']\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Conditions'] = le.fit_transform(df['Conditions'])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Precipitation'].values, df['Conditions'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código proporcionado, utilizamos la capa Dense de Keras para crear las capas de entrada y oculta de la red neuronal. La capa Dense es una capa completamente conectada, lo que significa que todas las neuronas de una capa están conectadas con las de la siguiente capa. En el caso de la capa de entrada, el parámetro input_dim se utiliza para especificar el número de entradas.\n",
    "\n",
    "En esta línea, \"Dense(10, input_dim=1, activation='relu')\" crea una capa densa con 10 neuronas y espera datos de entrada con 1 característica. La función de activación relu se utiliza para las neuronas de esta capa.\n",
    "\n",
    "Para la capa de salida, también usamos una capa Dense, pero la cantidad de neuronas corresponde a la cantidad de clases de salida y la función de activación suele ser softmax para problemas de clasificación de clases múltiples.\n",
    "\n",
    "En esta línea, \"Dense(len(le.classes_), activación='softmax')\" crea una capa de salida con una cantidad de neuronas igual a la cantidad de clases en la variable objetivo y usa la función de activación softmax para generar una distribución de probabilidad. sobre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de la red neuronal\n",
    "\n",
    "- Capa de entrada: 1 entrada (Precipitación), 10 neuronas, activación 'relu'\n",
    "- Capa oculta 1: 10 neuronas, activación 'relu'\n",
    "- Capa oculta 2: 10 neuronas, activación 'relu'\n",
    "- Capa de salida: número de neuronas igual al número de clases, activación con 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros conceptos clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Forward Propagation**: En forward propagation, los datos fluyen desde la capa de entrada a través de las capas ocultas hasta la capa de salida de la red neuronal. Cada neurona de una capa toma las salidas de la capa anterior (o los datos de entrada de la primera capa oculta), aplica pesos, agrega un bias y pasa el resultado a través de una función de activación. Luego, el resultado final se calcula en base a estos cálculos.\n",
    "\n",
    "- **Backpropagation**: Backpropagation es el proceso mediante el cual la red neuronal aprende y actualiza sus weights y biases. Después de la Forward Propagation, la salida de la red se compara con la salida esperada y se calcula el error. Luego, este error se propaga a través de la red, desde la capa de salida a la capa de entrada. Los weights y biases se actualizan de manera que se minimice este error. La cantidad en la que se actualizan las weights y los sesgos está determinada por la tasa de aprendizaje y el gradiente del error con respecto a las weights y los biases.\n",
    "\n",
    "- **Descenso de gradiente**: El descenso de gradiente es un algoritmo de optimización que se utiliza para minimizar la función de error moviéndose iterativamente en la dirección del descenso más pronunciado, que está determinado por el negativo del gradiente. En el contexto de las redes neuronales, el descenso de gradiente se utiliza durante la Backpropagation para actualizar los weights y bias para minimizar el error entre la salida de la red y la salida esperada. La tasa de aprendizaje determina el tamaño de los pasos dados en la dirección del descenso más pronunciado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activación\n",
    "\n",
    "- Primero, ¿qué es una función activadora? Es una función matemática aplicada a la salida de una neurona, y ¿cuáles son sus principales propósitos?\n",
    "\n",
    " 1. **No linealidad**: la mayoría de los datos del mundo real no son lineales, lo que significa que no se pueden representar con precisión con una línea recta. Las funciones de activación introducen no linealidad en la salida de una neurona. Esto permite que la red neuronal aprenda de los errores y realice representaciones complejas, lo que le permite resolver problemas no lineales.\n",
    "\n",
    " 2. **Normalización de salida**: Algunas funciones de activación como sigmoide o softmax garantizan que la salida de la neurona esté en un rango específico (entre 0 y 1 para sigmoide y sumando 1 para softmax). Esto puede resultar útil en determinados casos, como capas de salida para problemas de clasificación binaria o de clases múltiples.\n",
    "\n",
    " 3. **Habilitación de la BackPropagation**: Las funciones de activación y sus derivados se utilizan durante el proceso de BackPropagation cuando se actualizan los pesos.\n",
    "\n",
    " Las funciones de activación comunes incluyen la función sigmoidea, la función tangente hiperbólica (tanh), la unidad lineal rectificada (ReLU) y la función softmax. Cada uno tiene sus propias ventajas y se utiliza en capas específicas según los requisitos del modelo de red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existen varias funciones de activación comunes utilizadas en redes neuronales, cada una con sus propias características y casos de uso:\n",
    "\n",
    "1. **Función sigmoidea**: La función sigmoide genera un valor entre 0 y 1, lo que puede resultar útil para problemas de clasificación binaria. A menudo se utiliza en la capa de salida de una red de clasificación binaria. \n",
    "\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "2. **Función Tanh**: La función tangente hiperbólica, o tanh, genera un valor entre -1 y 1. Al igual que la función sigmoidea, es suave y diferenciable.\n",
    "\n",
    "```python\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "```\n",
    "\n",
    "3. **Función ReLU (Unidad lineal rectificada)**: La función ReLU genera la entrada directamente si es positiva; de lo contrario, genera cero. A menudo se usa en las capas ocultas de una red neuronal porque ayuda a la red a aprender patrones complejos. Sin embargo, puede causar un problema en el que las neuronas a veces pueden quedar atrapadas en el estado negativo.\n",
    "\n",
    "```python\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "```\n",
    "\n",
    "4. **Función Softmax**: la función softmax se usa a menudo en la capa de salida de una red neuronal para problemas de clasificación de clases múltiples. Convierte la salida de la red en una distribución de probabilidad entre múltiples clases.\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "```\n",
    "\n",
    "Cada una de estas funciones de activación tiene sus propios casos de uso y se utiliza en diferentes tipos de capas o diferentes tipos de redes neuronales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
